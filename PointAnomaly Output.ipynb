{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse, sys, os, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import powergrid_data\n",
    "from hmmlearn import hmm\n",
    "from datetime import datetime\n",
    "from scipy import stats, special\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from matplotlib.dates import YearLocator, MonthLocator, WeekdayLocator, DayLocator, HourLocator, AutoDateLocator, DateFormatter, AutoDateFormatter, date2num\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "from analysis import analyzer\n",
    "import warnings\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train Files Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = 'data/train/train.csv'\n",
    "test = 'data/test/test_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power_grid = powergrid_data.datasets(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pointAnomalyAnalyzer(power_grid, fullYear=None, fullMonth=[None], fullDay=None, threshold=1, n_comp=3, enable_meanStates = False):\n",
    "\n",
    "    analyze = analyzer(power_grid)\n",
    "\n",
    "    dates_columns = 'DateTime'\n",
    "    features_columns = ['Global_active_power']\n",
    "\n",
    "    analyze.time_series(dates_columns, features_columns)\n",
    "\n",
    "\n",
    "    ## If you modify year, month and day, make sure the above dates_columns is either Date or DateTime\n",
    "\n",
    "    ## Both test and train has data of year 2006, 2007 and 2008\n",
    "    Year = fullYear\n",
    "\n",
    "    ## Months takes a string of month full name e.g 'March'\n",
    "    Month = fullMonth\n",
    "    Day = fullDay\n",
    "\n",
    "    ## If you modify hour, minutes and seconds, make sure the above dates_columns is either DateTime or Time\n",
    "    ## Hour uses 24 hr clock\n",
    "    Hour = None\n",
    "    Minutes = None\n",
    "    Seconds = None\n",
    "\n",
    "    train_dates, train_features, test_dates, test_features = analyze.parser(year = Year, month = Month, day = Day, hour = Hour, minutes = Minutes, seconds = Seconds)\n",
    "\n",
    "\n",
    "    # Make an HMM instance and execute fit\n",
    "\n",
    "    init_probability = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "    translation_probability = np.array([\n",
    "        [0.7, 0.2, 0.1],\n",
    "        [0.3, 0.5, 0.2],\n",
    "        [0.3, 0.3, 0.4]]\n",
    "    )\n",
    "\n",
    "    model = hmm.GaussianHMM(n_components=n_comp, covariance_type=\"full\", tol = 0.1, n_iter=500, init_params=\"st\")\n",
    "    model.startprob_ = init_probability\n",
    "    model.transmat_ = translation_probability\n",
    "\n",
    "    model.fit(train_features)\n",
    "\n",
    "    train_state_seq = model.sample(test_features.size)[1]\n",
    "    test_state_seq = model.predict(test_features)\n",
    "\n",
    "    means = []\n",
    "    variance = []\n",
    "\n",
    "    for i in range(model.n_components):\n",
    "        means.append(model.means_[i].flatten()[0])\n",
    "        variance.append(model.covars_[i].flatten()[0])\n",
    "\n",
    "    standard_deviation = np.sqrt(variance)\n",
    "\n",
    "    anom = []\n",
    "    notanom = []\n",
    "    meanStates = []\n",
    "\n",
    "    test_observation = test_features.values\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(test_observation.size):\n",
    "        state = test_state_seq[i]\n",
    "        if (np.abs(means[state] - test_observation[i]) <= threshold):\n",
    "            notanom.append(True)\n",
    "        else:\n",
    "            notanom.append(False)\n",
    "        meanStates.append(means[state])\n",
    "\n",
    "    test_features_anomaly = np.copy(test_observation)\n",
    "    test_features_anomaly[notanom] = None\n",
    "#     0 is normal, 1 is anomally\n",
    "    AnomolyBinary = np.array(notanom) == False\n",
    "    \n",
    "    return AnomolyBinary.astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputFile(filename, threshold):\n",
    "    result = []\n",
    "    for year in power_grid._test_data['DateTime'].dt.year.unique():\n",
    "        result.extend(pointAnomalyAnalyzer(power_grid=power_grid, fullYear=year, threshold=threshold))\n",
    "    output = pd.DataFrame({'Anomaly Status' : result, 'Probability' : result})\n",
    "    output.to_csv(filename, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thresholds 0.5, 1, 1.25 ,1.5, 1.75 and 2\n",
    "\n",
    "outputFile('T0.5_'+'PointAnomaly.csv', 0.5)\n",
    "outputFile('T1_'+'PointAnomaly.csv', 1)\n",
    "outputFile('T1.25_'+'PointAnomaly.csv', 1.25)\n",
    "outputFile('T1.5_'+'PointAnomaly.csv', 1.5)\n",
    "outputFile('T1.75_'+'PointAnomaly.csv', 1.75)\n",
    "outputFile('T2_'+'PointAnomaly.csv', 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Windowing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "# Ref: https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator-in-python\n",
    "\n",
    "def window(seq, n=2):\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield np.array(result)\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield np.array(result)\n",
    "\n",
    "def pointAnomalyAnalyzerWindowing(power_grid, fullYear=None, fullMonth=[None], fullDay=None, threshold=1, n_comp=3, windowParam = 5, enable_meanStates = False):\n",
    "\n",
    "    analyze = analyzer(power_grid)\n",
    "\n",
    "    dates_columns = 'DateTime'\n",
    "    features_columns = ['Global_active_power']\n",
    "\n",
    "    analyze.time_series(dates_columns, features_columns)\n",
    "\n",
    "\n",
    "    ## If you modify year, month and day, make sure the above dates_columns is either Date or DateTime\n",
    "\n",
    "    ## Both test and train has data of year 2006, 2007 and 2008\n",
    "    Year = fullYear\n",
    "\n",
    "    ## Months takes a string of month full name e.g 'March'\n",
    "    Month = fullMonth\n",
    "    Day = fullDay\n",
    "\n",
    "    ## If you modify hour, minutes and seconds, make sure the above dates_columns is either DateTime or Time\n",
    "    ## Hour uses 24 hr clock\n",
    "    Hour = None\n",
    "    Minutes = None\n",
    "    Seconds = None\n",
    "\n",
    "    train_dates, train_features, test_dates, test_features = analyze.parser(year = Year, month = Month, day = Day, hour = Hour, minutes = Minutes, seconds = Seconds)\n",
    "\n",
    "\n",
    "    # Make an HMM instance and execute fit\n",
    "\n",
    "    init_probability = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "    translation_probability = np.array([\n",
    "        [0.7, 0.2, 0.1],\n",
    "        [0.3, 0.5, 0.2],\n",
    "        [0.3, 0.3, 0.4]]\n",
    "    )\n",
    "\n",
    "    model = hmm.GaussianHMM(n_components=n_comp, covariance_type=\"full\", tol = 0.1, n_iter=500, init_params=\"st\")\n",
    "    model.startprob_ = init_probability\n",
    "    model.transmat_ = translation_probability\n",
    "\n",
    "    #  window 5, 10, 15, and 20\n",
    "    for w in window(train_features.values.flatten(), windowParam):\n",
    "        train_window_features = w[:, np.newaxis]\n",
    "        model.fit(train_window_features)\n",
    "\n",
    "    train_state_seq = model.sample(test_features.size)[1]\n",
    "    test_state_seq = model.predict(test_features)\n",
    "\n",
    "    means = []\n",
    "    variance = []\n",
    "\n",
    "    for i in range(model.n_components):\n",
    "        means.append(model.means_[i].flatten()[0])\n",
    "        variance.append(model.covars_[i].flatten()[0])\n",
    "\n",
    "    standard_deviation = np.sqrt(variance)\n",
    "\n",
    "    anom = []\n",
    "    notanom = []\n",
    "    meanStates = []\n",
    "\n",
    "    test_observation = test_features.values\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(test_observation.size):\n",
    "        state = test_state_seq[i]\n",
    "        if (np.abs(means[state] - test_observation[i]) <= threshold):\n",
    "            notanom.append(True)\n",
    "        else:\n",
    "            notanom.append(False)\n",
    "        meanStates.append(means[state])\n",
    "\n",
    "    test_features_anomaly = np.copy(test_observation)\n",
    "    test_features_anomaly[notanom] = None\n",
    "#     0 is normal, 1 is anomally\n",
    "    AnomolyBinary = np.array(notanom) == False\n",
    "    \n",
    "    return AnomolyBinary.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# windowing 5, 10, 15, 20\n",
    "\n",
    "def outputFileForWindowing(filename, wparam):\n",
    "    result = []\n",
    "    for year in power_grid._test_data['DateTime'].dt.year.unique():\n",
    "        result.extend(pointAnomalyAnalyzerWindowing(power_grid=power_grid, fullYear=year, windowParam=wparam))\n",
    "    output = pd.DataFrame({'Anomaly Status' : result, 'Probability' : result})\n",
    "    output.to_csv(filename, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile('W5_'+'CollectiveAnomalyWindowing.csv', 5)\n",
    "outputFile('W10_'+'CollectiveAnomalyWindowing.csv', 10)\n",
    "outputFile('W15_'+'CollectiveAnomalyWindowing.csv', 15)\n",
    "outputFile('W20_'+'CollectiveAnomalyWindowing.csv', 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
