{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse, sys, os, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import powergrid_data\n",
    "from hmmlearn import hmm\n",
    "from datetime import datetime\n",
    "from scipy import stats, special\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from matplotlib import cm, pyplot as plt\n",
    "from matplotlib.dates import YearLocator, MonthLocator, WeekdayLocator, DayLocator, HourLocator, AutoDateLocator, DateFormatter, AutoDateFormatter, date2num\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "from analysis import analyzer\n",
    "import warnings\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train Files Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = 'data/train/train.csv'\n",
    "test = 'data/test/test_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power_grid = powergrid_data.datasets(train, test)\n",
    "analyze = analyzer(power_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Features and Dates\n",
    "\n",
    "> Note: Edit from here onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_columns = 'DateTime'\n",
    "features_columns = ['Global_active_power']\n",
    "\n",
    "analyze.time_series(dates_columns, features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign the scope to the parser to extract data\n",
    "> Scope can include years, months, days, hours, minutes and seconds to analyse data you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you modify year, month and day, make sure the above dates_columns is either Date or DateTime\n",
    "\n",
    "## Both test and train has data of year 2006, 2007 and 2008\n",
    "Year = 2007\n",
    "\n",
    "## Months takes a string of month full name e.g 'March'\n",
    "Month = ['May']\n",
    "Day = 5\n",
    "\n",
    "## If you modify hour, minutes and seconds, make sure the above dates_columns is either DateTime or Time\n",
    "## Hour uses 24 hr clock\n",
    "Hour = None\n",
    "Minutes = None\n",
    "Seconds = None\n",
    "\n",
    "train_dates, train_features, test_dates, test_features = analyze.parser(year = Year, month = Month, day = Day, hour = Hour, minutes = Minutes, seconds = Seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model and extracting max likelihood hidden state sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make an HMM instance and execute fit\n",
    "\n",
    "init_probability = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "translation_probability = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.3, 0.5, 0.2],\n",
    "    [0.3, 0.3, 0.4]]\n",
    ")\n",
    "\n",
    "model = hmm.GaussianHMM(n_components=3, covariance_type=\"full\", tol = 0.1, n_iter=500, init_params=\"st\")\n",
    "model.startprob_ = init_probability\n",
    "model.transmat_ = translation_probability\n",
    "\n",
    "model.fit(train_features)\n",
    "\n",
    "train_state_seq = model.sample(test_features.size)[1]\n",
    "test_state_seq = model.predict(test_features)\n",
    "\n",
    "means = []\n",
    "variance = []\n",
    "\n",
    "for i in range(model.n_components):\n",
    "    means.append(model.means_[i].flatten()[0])\n",
    "    variance.append(model.covars_[i].flatten()[0])\n",
    "\n",
    "standard_deviation = np.sqrt(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Anomaly Data and applying Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-155fc79b8072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_observation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_state_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtest_observation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mnotanom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "anom = []\n",
    "notanom = []\n",
    "meanStates = []\n",
    "\n",
    "test_observation = test_features.values\n",
    "\n",
    "# thresholds 0.5, 1, 1.25 ,1.5, 1.75 and 2\n",
    "\n",
    "for i in range(test_observation.size):\n",
    "    state = test_state_seq[i]\n",
    "    if (np.abs(means[state] - test_observation[i]) <= threshold):\n",
    "        notanom.append(True)\n",
    "    else:\n",
    "        notanom.append(False)\n",
    "    meanStates.append(means[state])\n",
    "\n",
    "test_features_anomaly = np.copy(test_observation)\n",
    "test_features_anomaly[notanom] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(test_dates, test_observation, 'go', markersize=4)\n",
    "plt.plot(test_dates, meanStates, 'b', markersize=4)\n",
    "plt.plot(test_dates, test_features_anomaly, 'ro', markersize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AnomolyBinary = AnomolyBinary.astype(int)\n",
    "\n",
    "# thresholds 0.5, 1, 1.25 ,1.5, 1.75 and 2\n",
    "threshold = 0.5\n",
    "result = []\n",
    "test_observation = test_features.values\n",
    "for i in range(test_observation.size):\n",
    "    state = test_state_seq[i]\n",
    "    if (np.abs(means[state] - test_observation[i]) < threshold):\n",
    "        result.append(AnomolyBinary[i])\n",
    "    else:\n",
    "        alpha = standard_deviation[state]\n",
    "        z_scores = (means[state] - test_observation[i])[0]/(alpha * threshold)\n",
    "        result.append(np.round(special.ndtr(-1*(z_scores)), 2))\n",
    "#         print(result[i])\n",
    "#         print(np.round(stats.norm.pdf(z_scores), 2))\n",
    "# p_values = stats.nospecialrm.sf(abs(z_scores)) #one-sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Anomaly' : AnomolyBinary, 'Probability' : result})\n",
    "\n",
    "output.to_csv('pointAnomaly.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
